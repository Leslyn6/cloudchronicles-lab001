{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1e0979",
   "metadata": {},
   "source": [
    "# üß† cloudChronicles Lab #001: Disaster Recovery Detective\n",
    "\n",
    "**Lab Type:** Idea  \n",
    "**Estimated Time:** 30‚Äì45 mins  \n",
    "**Skill Level:** Beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by printing your name to personalize the notebook\n",
    "your_name = \"\"\n",
    "print(f\"Welcome to the lab, {Lesly}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94d019",
   "metadata": {},
   "source": [
    "## üîç STAR Method Lab Prompt\n",
    "\n",
    "**Situation:**  \n",
    "[A regional outage has occurred in Google Cloud's us-central1 region. This implies that all resources within this region, including Compute Engine instances, Cloud SQL databases, Cloud Storage buckets, and other services, are currently inaccessible or severely degraded. Our primary objective is to restore critical business operations and data accessibility as quickly and efficiently as possible, with minimal data loss.]\n",
    "\n",
    "**Task:**  \n",
    "[The task is to execute a comprehensive disaster recovery plan that facilitates the failover of critical services to an alternate Google Cloud region (e.g., us-east1 or us-west1), ensures data consistency, and provides clear steps for recovery and post-incident analysis.]\n",
    "\n",
    "**Action:**  \n",
    "[Our DR plan involves a series of pre-configured automated and manual actions to achieve failover and recovery.
A regional outage in Google Cloud's us-central1 region is a critical incident that requires a robust and well-defined disaster recovery (DR) plan. As a Cloud Architect, I've outlined a STAR (Situation, Task, Action, Result) based plan, leveraging Google Cloud's native DR capabilities to ensure business continuity and minimize downtime.

STAR-Based Disaster Recovery Plan for Google Cloud us-central1 Regional Outage
S: Situation
A regional outage has occurred in Google Cloud's us-central1 region. This implies that all resources within this region, including Compute Engine instances, Cloud SQL databases, Cloud Storage buckets, and other services, are currently inaccessible or severely degraded. Our primary objective is to restore critical business operations and data accessibility as quickly and efficiently as possible, with minimal data loss.

T: Task
The task is to execute a comprehensive disaster recovery plan that facilitates the failover of critical services to an alternate Google Cloud region (e.g., us-east1 or us-west1), ensures data consistency, and provides clear steps for recovery and post-incident analysis.

A: Action
Our DR plan involves a series of pre-configured automated and manual actions to achieve failover and recovery.

Phase 1: Detection and Alerting

Google Cloud Monitoring & Operations Suite (formerly Stackdriver):
Custom Dashboards and Alerting Policies: Pre-configured dashboards will monitor key metrics across all services in us-central1, including network egress/ingress, CPU utilization, latency, and error rates.
Outage Detection: Anomalies in these metrics will trigger immediate alerts. Specific alert policies will be configured to detect us-central1 regional service health status changes.
Pub/Sub Integration: Alerts will be published to a dedicated Pub/Sub topic (e.g., dr-alerts).
Notification Channels: Subscribers to the dr-alerts Pub/Sub topic will include:
Email/SMS: Critical personnel (on-call engineers, DR team leads) will receive immediate notifications.
Chat/Collaboration Tools: Integration with tools like Slack or PagerDuty will ensure team awareness and coordination.
Cloud Functions/Cloud Run: Automated trigger for initial DR scripts.
Multi-Region Cloud Storage:

Pre-configuration: All critical data requiring high availability and disaster recovery is stored in multi-region Cloud Storage buckets (e.g., us multi-region location). This inherently provides data redundancy across multiple regions within the US.
Automatic Availability: In the event of a us-central1 outage, data stored in a us multi-region bucket remains accessible from us-east1, us-west1, and other available US regions without any manual intervention. Applications simply continue to access the bucket.
Application Re-pointing: While the data is available, applications deployed in us-east1 will naturally access the data from the multi-region bucket without needing a separate failover.
A regional outage in Google Cloud's us-central1 region is a critical incident that requires a robust and well-defined disaster recovery (DR) plan. As a Cloud Architect, I've outlined a STAR (Situation, Task, Action, Result) based plan, leveraging Google Cloud's native DR capabilities to ensure business continuity and minimize downtime.

STAR-Based Disaster Recovery Plan for Google Cloud us-central1 Regional Outage
S: Situation
A regional outage has occurred in Google Cloud's us-central1 region. This implies that all resources within this region, including Compute Engine instances, Cloud SQL databases, Cloud Storage buckets, and other services, are currently inaccessible or severely degraded. Our primary objective is to restore critical business operations and data accessibility as quickly and efficiently as possible, with minimal data loss.

T: Task
The task is to execute a comprehensive disaster recovery plan that facilitates the failover of critical services to an alternate Google Cloud region (e.g., us-east1 or us-west1), ensures data consistency, and provides clear steps for recovery and post-incident analysis.

A: Action
Our DR plan involves a series of pre-configured automated and manual actions to achieve failover and recovery.

Phase 1: Detection and Alerting

Google Cloud Monitoring & Operations Suite (formerly Stackdriver):
Custom Dashboards and Alerting Policies: Pre-configured dashboards will monitor key metrics across all services in us-central1, including network egress/ingress, CPU utilization, latency, and error rates.
Outage Detection: Anomalies in these metrics will trigger immediate alerts. Specific alert policies will be configured to detect us-central1 regional service health status changes.
Pub/Sub Integration: Alerts will be published to a dedicated Pub/Sub topic (e.g., dr-alerts).
Notification Channels: Subscribers to the dr-alerts Pub/Sub topic will include:
Email/SMS: Critical personnel (on-call engineers, DR team leads) will receive immediate notifications.
Chat/Collaboration Tools: Integration with tools like Slack or PagerDuty will ensure team awareness and coordination.
Cloud Functions/Cloud Run: Automated trigger for initial DR scripts.
Phase 2: Failover Strategy

Our strategy focuses on active-passive or warm-standby architectures for critical services, leveraging cross-region replication.

Cloud SQL Database Failover (Example: PostgreSQL, MySQL)

Pre-configuration: We have established cross-region Cloud SQL replicas. For instance, our primary us-central1 Cloud SQL instance has a read replica in us-east1.
Automated Failover Trigger (Cloud Functions/Cloud Run): Upon receiving a us-central1 outage alert via Pub/Sub, a Cloud Function (or Cloud Run service) will be triggered.
Promote Replica: This function will initiate the promotion of the us-east1 read replica to a standalone primary instance. This is a critical step that makes the replica writable.
DNS Update (Cloud DNS): The Cloud Function will also update the DNS record for the database endpoint to point to the new us-east1 primary instance's IP address. This is crucial for applications to connect to the new database.
Connection String Update: Applications will be configured to use a DNS-resolvable endpoint, making the failover transparent to them after the DNS propagation.
Multi-Region Cloud Storage:

Pre-configuration: All critical data requiring high availability and disaster recovery is stored in multi-region Cloud Storage buckets (e.g., us multi-region location). This inherently provides data redundancy across multiple regions within the US.
Automatic Availability: In the event of a us-central1 outage, data stored in a us multi-region bucket remains accessible from us-east1, us-west1, and other available US regions without any manual intervention. Applications simply continue to access the bucket.
Application Re-pointing: While the data is available, applications deployed in us-east1 will naturally access the data from the multi-region bucket without needing a separate failover.
Compute Engine/Kubernetes Engine Workload Failover:

Regional Managed Instance Groups (MIGs) or GKE Clusters: For stateless or containerized applications, we will have pre-provisioned, scaled-down (warm-standby) Compute Engine MIGs or GKE clusters in us-east1.
Deployment Automation (Cloud Deployment Manager/Terraform/Cloud Build):
Image Updates: Ensure that VM images or container images are synchronized across regions or pulled from a global registry (e.g., Artifact Registry).
Scaling Up: The DR automation (triggered by the Pub/Sub alert) will scale up the MIGs or GKE clusters in us-east1 to full production capacity.
Load Balancing (Global External HTTP(S) Load Balancing): Our global external HTTP(S) Load Balancer is already configured with backend services in both us-central1 (primary) and us-east1 (secondary).
Traffic Steering: Upon detecting the us-central1 outage, the Global Load Balancer will automatically or be manually reconfigured (via a script triggered by the Cloud Function) to direct all traffic to the us-east1 backend services. This provides immediate traffic failover for public-facing applications.
Internal Load Balancing: For internal services, we'd use cross-region Network Load Balancers or service mesh configurations if using GKE.
Pub/Sub Messaging:

Global Service: Pub/Sub is a global service, meaning it is not tied to a single region. Messages published to a topic can be consumed by subscribers in any region.
Subscriber Re-pointing: Our applications in us-east1 will simply subscribe to the existing Pub/Sub topics. No specific failover action is required for Pub/Sub itself, but ensuring the consumer applications are running in the alternate region is key.
Phase 3: Data Consistency and Recovery

Point-in-Time Recovery (Cloud SQL):
After the us-east1 replica is promoted, we will perform a point-in-time recovery if necessary, using transaction logs to bring the database to the most recent consistent state before the outage. This helps mitigate data loss during the brief failover window.
Backup and Restore (Cloud Storage/Cloud SQL):
Regular, automated backups are crucial. Even with multi-region replication, having backups in a separate secure location (e.g., a cross-region bucket, or a separate project) provides an additional layer of protection against logical corruption or accidental deletion.
In a worst-case scenario where replication is compromised, we would restore from the latest healthy backup.
Phase 4: Post-Outage Recovery and Reversion (When us-central1 is Restored)

Monitoring us-central1 Health: Continuously monitor the Google Cloud status page and our own monitoring dashboards for the restoration of us-central1 services.
Data Synchronization:
Cloud SQL: Once us-central1 is stable, we will establish a new replica from the us-east1 primary back to us-central1. This replica will catch up on all changes.
Application Data: For any application-specific data stored locally on VMs (which should be minimized in a cloud-native architecture), a synchronization plan (e.g., rsync, Cloud Storage FUSE) would be executed.
Planned Failback:
Pre-notification: Communicate the planned failback to stakeholders.
DNS/Load Balancer Update: Once the us-central1 replica is fully synchronized and promoted to primary, we will update DNS records and Global Load Balancer configurations to direct traffic back to us-central1. This will be a controlled, scheduled process to minimize impact.
Decommissioning us-east1 Resources: Scale down or de-provision the temporary resources in us-east1 to optimize costs.
Post-Mortem Analysis:
Incident Review: Conduct a thorough post-mortem to analyze the outage, the effectiveness of the DR plan, identify any gaps, and implement improvements.
Documentation Update: Update all DR documentation, runbooks, and playbooks based on lessons learned.
Testing: Schedule regular, simulated DR drills to ensure the plan remains effective and the team is proficient in its execution.
]\n",
    "\n",
    "**Expected Result:**  \n",
    "[A regional outage in Google Cloud's us-central1 region is a critical incident that requires a robust and well-defined disaster recovery (DR) plan. As a Cloud Architect, I've outlined a STAR (Situation, Task, Action, Result) based plan, leveraging Google Cloud's native DR capabilities to ensure business continuity and minimize downtime.

STAR-Based Disaster Recovery Plan for Google Cloud us-central1 Regional Outage
S: Situation
A regional outage has occurred in Google Cloud's us-central1 region. This implies that all resources within this region, including Compute Engine instances, Cloud SQL databases, Cloud Storage buckets, and other services, are currently inaccessible or severely degraded. Our primary objective is to restore critical business operations and data accessibility as quickly and efficiently as possible, with minimal data loss.

T: Task
The task is to execute a comprehensive disaster recovery plan that facilitates the failover of critical services to an alternate Google Cloud region (e.g., us-east1 or us-west1), ensures data consistency, and provides clear steps for recovery and post-incident analysis.

A: Action
Our DR plan involves a series of pre-configured automated and manual actions to achieve failover and recovery.

Phase 1: Detection and Alerting

Google Cloud Monitoring & Operations Suite (formerly Stackdriver):
Custom Dashboards and Alerting Policies: Pre-configured dashboards will monitor key metrics across all services in us-central1, including network egress/ingress, CPU utilization, latency, and error rates.
Outage Detection: Anomalies in these metrics will trigger immediate alerts. Specific alert policies will be configured to detect us-central1 regional service health status changes.
Pub/Sub Integration: Alerts will be published to a dedicated Pub/Sub topic (e.g., dr-alerts).
Notification Channels: Subscribers to the dr-alerts Pub/Sub topic will include:
Email/SMS: Critical personnel (on-call engineers, DR team leads) will receive immediate notifications.
Chat/Collaboration Tools: Integration with tools like Slack or PagerDuty will ensure team awareness and coordination.
Cloud Functions/Cloud Run: Automated trigger for initial DR scripts.
Phase 2: Failover Strategy

Our strategy focuses on active-passive or warm-standby architectures for critical services, leveraging cross-region replication.

Cloud SQL Database Failover (Example: PostgreSQL, MySQL)

Pre-configuration: We have established cross-region Cloud SQL replicas. For instance, our primary us-central1 Cloud SQL instance has a read replica in us-east1.
Automated Failover Trigger (Cloud Functions/Cloud Run): Upon receiving a us-central1 outage alert via Pub/Sub, a Cloud Function (or Cloud Run service) will be triggered.
Promote Replica: This function will initiate the promotion of the us-east1 read replica to a standalone primary instance. This is a critical step that makes the replica writable.
DNS Update (Cloud DNS): The Cloud Function will also update the DNS record for the database endpoint to point to the new us-east1 primary instance's IP address. This is crucial for applications to connect to the new database.
Connection String Update: Applications will be configured to use a DNS-resolvable endpoint, making the failover transparent to them after the DNS propagation.
Multi-Region Cloud Storage:

Pre-configuration: All critical data requiring high availability and disaster recovery is stored in multi-region Cloud Storage buckets (e.g., us multi-region location). This inherently provides data redundancy across multiple regions within the US.
Automatic Availability: In the event of a us-central1 outage, data stored in a us multi-region bucket remains accessible from us-east1, us-west1, and other available US regions without any manual intervention. Applications simply continue to access the bucket.
Application Re-pointing: While the data is available, applications deployed in us-east1 will naturally access the data from the multi-region bucket without needing a separate failover.
Compute Engine/Kubernetes Engine Workload Failover:

Regional Managed Instance Groups (MIGs) or GKE Clusters: For stateless or containerized applications, we will have pre-provisioned, scaled-down (warm-standby) Compute Engine MIGs or GKE clusters in us-east1.
Deployment Automation (Cloud Deployment Manager/Terraform/Cloud Build):
Image Updates: Ensure that VM images or container images are synchronized across regions or pulled from a global registry (e.g., Artifact Registry).
Scaling Up: The DR automation (triggered by the Pub/Sub alert) will scale up the MIGs or GKE clusters in us-east1 to full production capacity.
Load Balancing (Global External HTTP(S) Load Balancing): Our global external HTTP(S) Load Balancer is already configured with backend services in both us-central1 (primary) and us-east1 (secondary).
Traffic Steering: Upon detecting the us-central1 outage, the Global Load Balancer will automatically or be manually reconfigured (via a script triggered by the Cloud Function) to direct all traffic to the us-east1 backend services. This provides immediate traffic failover for public-facing applications.
Internal Load Balancing: For internal services, we'd use cross-region Network Load Balancers or service mesh configurations if using GKE.
Pub/Sub Messaging:

Global Service: Pub/Sub is a global service, meaning it is not tied to a single region. Messages published to a topic can be consumed by subscribers in any region.
Subscriber Re-pointing: Our applications in us-east1 will simply subscribe to the existing Pub/Sub topics. No specific failover action is required for Pub/Sub itself, but ensuring the consumer applications are running in the alternate region is key.
Phase 3: Data Consistency and Recovery

Point-in-Time Recovery (Cloud SQL):
After the us-east1 replica is promoted, we will perform a point-in-time recovery if necessary, using transaction logs to bring the database to the most recent consistent state before the outage. This helps mitigate data loss during the brief failover window.
Backup and Restore (Cloud Storage/Cloud SQL):
Regular, automated backups are crucial. Even with multi-region replication, having backups in a separate secure location (e.g., a cross-region bucket, or a separate project) provides an additional layer of protection against logical corruption or accidental deletion.
In a worst-case scenario where replication is compromised, we would restore from the latest healthy backup.
Phase 4: Post-Outage Recovery and Reversion (When us-central1 is Restored)

Monitoring us-central1 Health: Continuously monitor the Google Cloud status page and our own monitoring dashboards for the restoration of us-central1 services.
Data Synchronization:
Cloud SQL: Once us-central1 is stable, we will establish a new replica from the us-east1 primary back to us-central1. This replica will catch up on all changes.
Application Data: For any application-specific data stored locally on VMs (which should be minimized in a cloud-native architecture), a synchronization plan (e.g., rsync, Cloud Storage FUSE) would be executed.
Planned Failback:
Pre-notification: Communicate the planned failback to stakeholders.
DNS/Load Balancer Update: Once the us-central1 replica is fully synchronized and promoted to primary, we will update DNS records and Global Load Balancer configurations to direct traffic back to us-central1. This will be a controlled, scheduled process to minimize impact.
Decommissioning us-east1 Resources: Scale down or de-provision the temporary resources in us-east1 to optimize costs.
Post-Mortem Analysis:
Incident Review: Conduct a thorough post-mortem to analyze the outage, the effectiveness of the DR plan, identify any gaps, and implement improvements.
Documentation Update: Update all DR documentation, runbooks, and playbooks based on lessons learned.
Testing: Schedule regular, simulated DR drills to ensure the plan remains effective and the team is proficient in its execution.
R: Result
The successful execution of this STAR-based disaster recovery plan will result in:

Minimized Downtime: Critical applications and data will be restored and accessible within minutes to a few hours, depending on the service, significantly reducing the impact of the us-central1 regional outage.
Reduced Data Loss (RPO): Leveraging Cloud SQL replicas and multi-region Cloud Storage ensures a near-zero Recovery Point Objective (RPO) for critical data, meaning very little to no data loss.
High Availability and Business Continuity: Core business functions will continue to operate from an alternate region, maintaining service availability for users and customers.
Cost Optimization: By using warm-standby resources and scaling up only during an event, we balance high availability with cost efficiency.
Improved Resilience: The exercise will validate our DR capabilities and provide valuable insights for continuous improvement of our cloud architecture and operational procedures.
Clear Communication: Pre-defined communication channels and protocols will ensure all stakeholders are informed throughout the incident and recovery process.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b221d",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Your Assignment\n",
    "\n",
    "_Use this section to complete your deliverable:_\n",
    "\n",
    "```markdown\n",
    "(Example Format)\n",
    "\n",
    "- **Primary Region**: us-central1  \n",
    "- **Backup Location**: us-east1  \n",
    "- **Failover Trigger**: Load balancer health check + Pub/Sub alert  \n",
    "- **Redundancy Services**:  \n",
    "   - Cloud SQL with failover  \n",
    "   - Cloud Storage versioning  \n",
    "   - Cloud Functions for health monitoring  \n",
    "- **Backup Schedule**: Every 6 hours, daily export to multi-region bucket  \n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
